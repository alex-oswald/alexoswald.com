{"/about/":{"data":{"":"","#":"\nI\u0026rsquo;m Alex, and I\u0026rsquo;m a Full Stack Software Engineer \u0026amp; Electrical Engineer. I\u0026rsquo;m a Software Engineer at Microsoft helping make Windows better on ARM devices.\nAlong with my day job, I run Oswald Technologies, LLC in my spare time. The business provided TireDispatcher from 2013 to October 2020. TireDispatcher is Software as a Service (SaaS) that helps commercial tire companies keep track of their mobile services. Some customers included GCR Tires \u0026amp; Service, Continental Tire, Les Schwab Tire Center, Kal Tire, Conlan Tire, and Best One Tire.\nEducation\rBachelor of Science in Electronics Engineering Technology "},"title":"About Me"},"/blog/2020-10-28-creating-my-static-blog-site/":{"data":{"":"","create-a-build-pipeline#Create a build pipeline":"","create-a-release-pipeline#Create a Release Pipeline":"","create-a-resource-group-in-azure#Create a resource group in Azure":"","create-project-in-azure-devops#Create project in Azure DevOps":"","prerequisites#Prerequisites":"","references#References":"","setup-custom-domain#Setup custom domain":"","setup-http-rules-for-the-cdn-endpoint#Setup HTTP rules for the CDN endpoint":"","setup-storage-account-in-azure#Setup storage account in Azure":"","thank-you#Thank you":"","updates#Updates":"I\u0026rsquo;ve always wanted to create my own blog to share my solutions to different coding problems I\u0026rsquo;ve experienced over the years. I\u0026rsquo;ve finally made time to create one, so here is how I did it.\nI started with a few requirements:\nStatic site generator - I wanted to use a static site generator for its simplicity and speed benefits. Custom domain - Use the domain I purchased: alexoswald.com Fast \u0026amp; secure - It needs to be secure and fast, with caching. Cheap, less than $50 per year - The domain is $20 a year from GoDaddy with privacy. That leaves $30 for hosting and security. I am a Microsoft fan and .NET fanatic so I will most always utilize their tools. Another few requirements regarding personal preferences:\nAzure DevOps for version control Azure Pipelines for builds and release deployments Azure CDN In this guide, we will setup Azure to host a blog via Azure CDN. We will use Jekyll to generate our static site. Since we don\u0026rsquo;t want to deal with installing Ruby or Jekyll, we will develop the site in Visual Studio Code using dev containers. With dev containers, we can develop inside a container that already has Ruby and Jekyll installed.\nPrerequisites\rAzure subscription\nAzure DevOps\nA custom domain\nVisual Studio Code with the Remote - Containers extension\nDocker Desktop\nCreate a resource group in Azure\rCreate a resource group in Azure that will hold all of the blogs resources. In this instance, I\u0026rsquo;m naming the resource group AlexsBlogResourceGroup.\nSetup storage account in Azure\rCreate a storage account in Azure\nIn the storage account, go to Static website. Set the Static website option to Enabled.\nSet Index document name to index.html (generated by Jekyll).\nSet Error document path to 404.html (generated by Jekyll).\nSave the change.\nThe Primary endpoint will be used in the next step.\nVisit Settings \u0026gt; Access keys.\nkey1 will be used in the Azure pipeline to access the storage account.\nCreate Azure CDN endpoint\rWhile viewing the storage account resource, under Blob service, go to Azure CDN.\nFrom here, we are going to create a new Endpoint.\nFor CDN profile, select Create new, with the name of your choosing.\nSet the Pricing tier to Premium Verizon so we can set HTTP rules later.\nSet the Origin hostname to the static website Primary endpoint from above.\nEnter the CDN endpoint name.\nClick Create.\nNavigate to the newely created resource.\nNavigate to Settings \u0026gt; Origin and deselect HTTP. We only want to allow HTTPS.\nCreate project in Azure DevOps\rLogin to your DevOps account: https://dev.azure.com/\nAdd a new project, making sure Version control is set to Git.\nNavigate to Repos \u0026gt; Files.\nNow we are going to Clone in VS Code for next steps.\nUse Jekyll to generate the site inside a Dev Container\rCheck out Microsoft\u0026rsquo;s dev containers tutorial\nWe have an empty repo now open in VS Code. To get started, lets get our dev container running. To do this, open the Command Palette and run the task Remote-Containers: Open Folder in Container.\nSelect your repo folder. Once you do this, VS Code will ask you to choose the correct configuration files.\nWe will select Jeykll.\nThis will automatically create the dev containers config file, Dockerfile, and VS Code tasks for Jekyll.\nI\u0026rsquo;ve made a few small modifications to the configuration files.\nIn the dev container Dockerfile lets uncomment the section to update and install other packages. I\u0026rsquo;d like to install git as well.\nRUN apt-get update \u0026amp;\u0026amp; export DEBIAN_FRONTEND=noninteractive \\\r\u0026amp;\u0026amp; apt-get -y install git\rWe also want the bundler to install packages after the dev container is created. To do this, open devcontainer.json and uncomment the line \u0026quot;postCreateCommand\u0026quot;: \u0026quot;bundle install\u0026quot;,.\nIf you don\u0026rsquo;t already have the proper .gitignore file in your project directory, create one with the following contents (this file was generated for me by Azure Devops):\n_site\r.sass-cache\r.jekyll-cache\r.jekyll-metadata\rYour development environment is now running in a container so lets get down to business and create our Jekyll site.\nOpen a terminal window. Since our dev environment is running in a container with Linux, we are presented with a bash terminal.\nCreate a new Jekyll site in the current folder.\njekyll new ./\rJekyll will create the site, and a Gemfile. Then it will run bundle install to install the gems specified in the Gemfile.\nYour projects file structure should now be similar to the following. Items in green were created by Jekyll.\nNow we are going to build and run the site to check it out. We can manually execute the commands, or use one of the two VS Code tasks that were created for us in tasks.json. Lets run the Serve task. Open the Command Palette and search/click Tasks: Run Task, then click Serve.\nAfter your site is built, navigate to http://localhost:4000/ in a browser.\nVoil√†, we now have a generated static website!\nCreate a build pipeline\rWe are going to create a build pipeline in Azure DevOps so the site is built each time a push is made to the master or a tag is created. The built site is stored as an Artifact and can be used in other pipelines.\nNavigate to Pipelines \u0026gt; Pipelines and click Create Pipeline.\nYou will be asked where your code is. Select Azure Repos Git.\nThen select your repository.\nChoose the Starter pipeline. Now replace its contents with this yaml:\n# azure-pipelines.yaml # Trigger pipeline on any push to master and any tag creations trigger: branches: include: - master tags: include: [\u0026#39;*\u0026#39;] # Build on Ubuntu pool pool: vmImage: \u0026#39;ubuntu-latest\u0026#39; steps: # Use Ruby - task: UseRubyVersion@0 displayName: \u0026#39;Use Ruby \u0026gt;= 2.5\u0026#39; inputs: versionSpec: \u0026#39;\u0026gt;= 2.5\u0026#39; # Install Jekyll - script: \u0026#39;gem install jekyll bundler\u0026#39; displayName: \u0026#39;Install Jekyll and bundler\u0026#39; # Install Jekyll dependency - script: \u0026#39;bundle install\u0026#39; displayName: \u0026#39;Install Gems\u0026#39; # Run Jekyll and build the site - script: \u0026#39;bundle exe jekyll build\u0026#39; displayName: Build # Copy packaged site to the staging directory for publishing - task: CopyFiles@2 displayName: \u0026#39;Copy \u0026#34;_site\u0026#34; to staging directory\u0026#39; inputs: SourceFolder: \u0026#39;_site\u0026#39; TargetFolder: \u0026#39;$(Build.ArtifactStagingDirectory)\u0026#39; # Publish the artifact - task: PublishBuildArtifacts@1 displayName: \u0026#39;Publish Artifact: _site\u0026#39; inputs: ArtifactName: \u0026#39;_site\u0026#39;\rClick Save and run, specify commit message and options, then click Save and run again.\nCreate a Release Pipeline\rOur site is built and stored in DevOps as an Artifact, waiting to be deployed. We will create a release pipeline that will deploy our site to Azure.\nNavigate to Pipelines \u0026gt; Releases and click New pipeline.\nSelect Empty job\nClick X to close the Stage pane.\nClick Add an artifact.\nChoose Build, select your Project, then select the build pipeline we created previously as the Source. Click Add.\nWe have now added the artifact so it can be used by the pipeline.\nUnder Stage 1, click 1 job, 0 task.\nAdd tasks\rWe have two new tasks to add.\nThe first task is to Sync the files in the Artifact with your static website container in the Azure Storage account specified. Click the + to add a new task. Add an Azure CLI task.\nConfigure the task:\nUpdate Display name to Sync files Set Azure Resource Manager connection Set Script Type to Batch Set Script Location to Inline script. Set the Inline Script to the following: Set the Working Directory. Click the \u0026hellip; button and select the build folder. az storage blob sync --source $(source) --container $(containerName) --account-name $(storageAccount) --account-key $(key)\rTask example yaml:\n# This code syncs with the storage blob you specify # It won\u0026#39;t delete all files, then upload. It will # upload only what is needed, and only delete what # is needed. steps: - task: AzureCLI@1 displayName: \u0026#39;Sync files\u0026#39; inputs: azureSubscription: \u0026#39;$(subscription)\u0026#39; scriptLocation: inlineScript inlineScript: \u0026#39;az storage blob sync --source $(source) --container $(containerName) --account-name $(storageAccount) --account-key $(key)\u0026#39; workingDirectory: \u0026#39;$(System.DefaultWorkingDirectory)/_MyBlog\u0026#39;\rThe second task is to Purge the CDN of all cached files so the new site is pulled and cached. Add another Azure CLI task.\nConfigure the task:\nUpdate Display name to Purge CDN Set Azure Resource Manager connection Set Script Type to Batch Set Script Location to Inline script. Set the Inline Script to the following: az cdn endpoint purge --profile-name $(cdnProfile) --content-paths /* --name $(endpointName) --resource-group $(resourceGroup)\rTask example yaml:\n# Purges the CDN\u0026#39;s cache so it has to fetch new (updated) # content from the storage container steps: - task: AzureCLI@1 displayName: \u0026#39;Purge CDN\u0026#39; inputs: azureSubscription: \u0026#39;$(subscription)\u0026#39; scriptLocation: inlineScript inlineScript: \u0026#39;az cdn endpoint purge --profile-name $(cdnProfile) --content-paths /* --name $(endpointName) --resource-group $(resourceGroup)\u0026#39;\rAdd variables\rBoth of our tasks use multiple variables we need to define. While still editing Release, click Variables.\nAdd the following variables:\nSync files\nsource: Folder name in the artifact that contains the site, _site for Jekyll\ncontainerName: $web (set by Azure)\nstorageAccount: The name of your storage account\nkey: Access key to your storage account\nPurge CDN\ncdnProfile: The name of the CDN Profile used by the Endpoint resource\nendpointName: The name of the Endpoint\nresourceGroup: The name of the Resource Group that contains the Endpoint\nMake sure to Save your pipeline\nCreate a release\rIn the top right next to the Save button you just clicked, click Create release, after the dialog shows, click Create.\nThis process may take a few minutes. Once complete, open a browser and navigate to the storage accounts Primary endpoint you wrote down previously. You can also access the site via the CDN endpoint we created. The site will be https://endpointname.azureedge.net.\nYour site should now be deployed! Horray!\nSetup custom domain\rAdd DNS records to domain\rIn order to setup our custom domains Azure needs to verify you own the domain so we need to setup some CNAME records that Azure can verify.\nGo to your domain providers DNS management page.\nAdd a CNAME with Host cdnverify.www and Value cdnverify.endpointname.azureedge.net.\nAdd a CNAME with Host www and Value endpointname.azureedge.net.\nI\u0026rsquo;m using GoDaddy for my domains.\nAdd custom domain to the CDN endpoint\rNavigate to the Azure CDN Endpoint resource.\nNavigate to Settings \u0026gt; Custom domains \u0026gt; + Custom domain\nFill in your www subdomain. If Azure is able to verify your CNAME record you will see the green check, otherwise there will be a red X.\nNow we need to enable HTTPS for the hostname.\nClick your custom domain.\nSet Custom domain HTTPS to On.\nI am using Azure\u0026rsquo;s free certificates so I set Certificate management type to CDN managed.\nThen click Save.\nIn my experience, the certificate deployment process can take 6-8 hours to complete.\nOnce complete, the screen should look like the below.\nIMPORTANT\nI was able to get my apex domain added as a custom domain to my Azure CDN endpoint, but Azure no longer supports using their free SSL certificates for apex domains. The solution I decided on involves setting up Azure CDN using the www subdomain and then forwarding requests to the apex domain to the www subdomain. While I personally dislike having to use the www subdomain at all, it is providing me a free SSL certificate.\nHere is the error I got trying to add an endpoint for the apex domain.\nSetup HTTP rules for the CDN endpoint\rThere are some HTTP rules we want to set up to provide for better security and a better experience.\nNavigate to your CDN profile resource and click Manage.\nThen go to HTTP Large \u0026gt; Rules Engine V4.0.\nI won\u0026rsquo;t go into detail on how to setup policys, as you can check the documentation.\nWe will prefix the Source to /80 since we are using a custom domain. Here are the docs. The next 5 characters is your account id. It should be in the top right corner in parenthesis of the CDN management page. The next part of the source is your CDN endpoint\u0026rsquo;s name.\nRedirect HTTP to HTTPS\rAny request to http will be redirected to use https.\nAdd a new rule.\nIf Request Request Scheme matches http.\nAdd feature.\nFeature URL URL Redirect Source /80{act}/{endpoint-name}/(.*) Destination https://www.alexoswald.com/$1 Code 301 Caching\rCache content for 1 year on the server and 1 day on the client. Each time I update the site I purge the CDN, so I want the internal cache to be longer than the longest potential time between blog posts.\nAdd a new rule.\nIf General Always.\nAdd feature.\nFeature Caching Default Internal Max-Age Status 200 Value 525600 Units minutes Add feature.\nFeature Caching External Max-Age Value 86400 Units seconds HSTS Header\rInstructs the browser to forward to https if needed.\nAdd a new rule.\nIf General Always.\nAdd feature.\nFeature Headers Modify Client Response Header Action Append Name Strict-Transport-Security Value max-age=31536000; includeSubDomains Redirect Root to WWW\rNOTE\nSince we are not able to add the free certificate to an endpoint with a custom domain that is an apex domain, we didn\u0026rsquo;t setup an endpoint for the apex domain. Therefore, we are not able to use HTTP Rules to redirect from root to www. Some other blogs I read did this, but it must of been before they disabled certificates for apex domains, or they purchased their own certificate. Since I am using GoDaddy, I will use their DNS management to forward root domain requests to www.alexoswald.com. This forwards http://alexoswald.com and https://alexoswald.com to https://alexoswald.com.\nOther headers\rI think it is important to add other security headers so I\u0026rsquo;ve made sure to have at least an A rating on Securityheaders.com. I suggest following the suggestions there to add other headers and improve your security.\nReferences\rHere are some other developer blog posts that helped me complete this project.\nArlan Nugara\u0026rsquo;s blog post helped me add the apex domain to the CDN, though I couldn\u0026rsquo;t use it because I can\u0026rsquo;t get the free certificate.\nGlenn Price\u0026rsquo;s blog post helped me because at first my site wasn\u0026rsquo;t being served, and it turned out to be because I had forgot to set the Index document name and Error document path in the static site setup.\nDuncan Mackenzie\u0026rsquo;s blog post helped me understand how to add security headers and why the order matters.\nSecurityHeaders.com/ is a great site that lets you check your HTTP headers and rates your sites security. The author, Scott Helme has some great blog posts on headers as well.\nThank you\rI hope you enjoyed my first post. I learned a lot reading other developers blogs and documentation so I hope you did too.\nIf you have any comments, please feel free to email me at alex@oswaldtechnologies.com.\n-Alex\nUpdates\r2022-08-06\naz storage blob sync no longer requires the argument --auth-mode key ","use-jekyll-to-generate-the-site-inside-a-dev-container#Use Jekyll to generate the site inside a Dev Container":""},"title":"Creating a Static Blog with Jekyll, Dev Containers \u0026 Azure CDN!"},"/blog/2021-08-08-pi-hole-in-docker-on-a-pi/":{"data":{"":"","copy-files-to-the-pi#Copy files to the Pi":"","create-our-files#Create our files":"","install-docker#Install docker":"","install-docker-compose#Install docker compose":"","on-the-pi#On the Pi\u0026hellip;":"","prepare-the-raspberry-pi-sd-card#Prepare the Raspberry Pi SD card":"","prerequisites#Prerequisites":"","remote-into-the-pi#Remote into the Pi":"","start-the-dhcp-server#Start the DHCP server":"","success#Success":"","thank-you#Thank you":"I\u0026rsquo;ve been wanting to setup Pi-Hole in my house for quite some time and I finally got around to getting it done. It has been working great so far, blocking an average of 15-20% of requests. This blog post explains how I\u0026rsquo;m running Pi-Hole via Docker on a Raspberry Pi.\nI\u0026rsquo;m dedicating a Raspberry Pi 4 to the task. With all the Docker work I\u0026rsquo;ve been doing lately, I decided to run Pi-Hole in a container, and you can too.\nVPN Notice\nIf you use a VPN, remember to disable options to only use their DNS servers. If you do not, devices connected to a VPN using their DNS servers will not see the benefit of Pi-Hole.\nPrerequisites\rRaspberry Pi 4 Model B Micro SD card 8GB or larger SSH public key read Raspberry Pi\u0026rsquo;s docs how Prepare the Raspberry Pi SD card\rFirst, we are going to prepare the SD card by downloading/installing Raspberry Pi Imager, and imaging our card. We will set up Wifi and SSH through the imager so we don\u0026rsquo;t need to do it manually later.\nhttps://www.raspberrypi.org/software/\nRun the imager once installed.\nImage with Raspberry Pi OS Lite (32-bit)\nPress Ctrl+Shift+X to open Advanced options\nSet a hostname, I named mine rp-pihole Enter your ssh public key Configure Wifi Set locale settings Install the SD card into the Pi and power it on.\nRemote into the Pi\rThe Pi should be fully booted after a few minutes. Now we can SSH in.\nssh pi@rp-pihole\rWhere pi is the username and rp-pihole is the hostname.\nNow that we are in, we need to update \u0026amp; upgrade, then install docker \u0026amp; docker compose.\nsudo apt-get update \u0026amp;\u0026amp; sudo apt-get upgrade\rInstall docker\rDownload the Docker installation script \u0026amp; execute it.\nhttps://github.com/docker/docker-install/blob/master/install.sh\ncurl -fsSL https://get.docker.com -o get-docker.sh \u0026amp;\u0026amp; sudo sh get-docker.sh\rWe can delete the file now.\nrm get-docker.sh\rSetup non-root user to Docker group so we can execute docker without sudo. Logout and back in so changes take affect.\nsudo usermod -aG docker pi logout\rCheck the version to verify installation. If you don\u0026rsquo;t see info for the Docker Engine, you may need to reboot the device.\ndocker version\rYou will see something similar to:\nClient:\rCloud integration: 1.0.17\rVersion: 20.10.7\rAPI version: 1.41\rGo version: go1.16.4\rGit commit: f0df350\rBuilt: Wed Jun 2 12:00:56 2021\rOS/Arch: windows/amd64\rContext: desktop-linux\rExperimental: true\rServer: Docker Engine - Community\rEngine:\rVersion: 20.10.7\rAPI version: 1.41 (minimum version 1.12)\rGo version: go1.13.15\rGit commit: b0f5bc3\rBuilt: Wed Jun 2 11:54:58 2021\rOS/Arch: linux/amd64\rExperimental: false\rcontainerd:\rVersion: 1.4.6\rGitCommit: d71fcd7d8303cbf684402823e425e9dd2e99285d\rrunc:\rVersion: 1.0.0-rc95\rGitCommit: b9ee9c6314599f1b4a7f497e1f1f856fe433d3b7\rdocker-init:\rVersion: 0.19.0\rGitCommit: de40ad0\rInstall docker compose\rsudo apt install -y python3-pip libffi-dev sudo pip3 install docker-compose\rCheck the version to verify installation.\ndocker-compose version\rYou will see something similar to:\ndocker-compose version 1.29.2, build 5becea4c\rdocker-py version: 5.0.0\rCPython version: 3.9.0\rOpenSSL version: OpenSSL 1.1.1g 21 Apr 2020\rCreate our files\rOn our dev machine, we will create two files:\ndocker-compose.yml - Our compose file .env - The environment variables file Create the environment variables file\rCreate a .env file with the contents:\nPIHOLE_IPADDRESS=10.10.10.2\rPIHOLE_HOST=pi.hole\rPIHOLE_TIMEZONE=America/Los_Angeles\rPIHOLE_WEBPASSWORD=\rPIHOLE_IPADDRESS - This is the IP address of the Pi running Pi-Hole (it should be statically reserved by your router).\nPIHOLE_HOST - The host used to access the admin console.\nPIHOLE_TIMEZONE - Your timezone.\nPIHOLE_WEBPASSWORD - Password for the admin console.\nCreate the compose file\rCreate a docker-compose.yml file with the contents:\nversion: \u0026#39;3.9\u0026#39; services: pihole: container_name: \u0026#39;pihole\u0026#39; image: \u0026#39;pihole/pihole:latest\u0026#39; restart: \u0026#39;unless-stopped\u0026#39; network_mode: host environment: ServerIP: \u0026#39;${PIHOLE_IPADDRESS}\u0026#39; VIRTUAL_HOST: \u0026#39;${PIHOLE_HOST}\u0026#39; TZ: \u0026#39;${PIHOLE_TIMEZONE}\u0026#39; WEBPASSWORD: \u0026#39;${PIHOLE_WEBPASSWORD}\u0026#39; DNSMASQ_LISTENING: all DNS1: \u0026#39;127.0.0.1\u0026#39; DNS2: \u0026#39;1.1.1.1\u0026#39; volumes: - \u0026#39;pihole-data:/etc/pihole/\u0026#39; - \u0026#39;pihole-dnsmasq:/etc/dnsmasq.d/\u0026#39; cap_add: - \u0026#39;NET_ADMIN\u0026#39; volumes: pihole-data: pihole-dnsmasq:\rThe above configuration allows Pi-Hole\u0026rsquo;s DHCP server to work properly on my network.\nCopy files to the Pi\rOpen a terminal in the directory of the files you just created. Run secure copy to copy the .env and docker-compose.yml files to the Pi.\nscp -r .\\.env .\\docker-compose.yml pi@rp-pihole:/home/pi\rOn the Pi\u0026hellip;\rThe next step is simple, fire up Pi-Hole!\nRun docker compose\ndocker-compose up -d\rCheck container status or Pi-Holes logs if you wish:\ndocker ps\rdocker logs pihole\rGive it a minute to start up and you should now be able to visit the admin console in a web browser. http://pi.hole\nStart the DHCP server\rTo enable the DHCP server you must first disable your routers DHCP server.\nIn Pi-Hole\u0026rsquo;s admin console, go to Settings, then to the DHCP tab.\nCheck DHCP server enabled and click Save.\nOnce enabled, it may take some time for devices IP address leases to expire, and ask Pi-Hole\u0026rsquo;s DHCP server for a new IP address. Once they do, the devices will show up in the DHCP leases section of the page.\nSuccess\rOnce devices start requiring IP addresses from Pi-Hole\u0026rsquo;s DHCP server, Pi-Hole will start doing its magic and blocking a ton of ad and metric sites.\nTroubleshooting\rI was having an issue where my pihole device would loose its DHCP reservartion after 24 hours and the device wouldn\u0026rsquo;t assign itself a static IP so it would get assigned a random IP which caused DNS to stop working on the network and everything connected to break. The Raspberry Pi running pihole may need some additional configuration to make sure it assigns itself with the same static IP. To do this, edit dhcpcd.conf with the following command:\nsudo nano /etc/dhcpcd.conf\rAdd the following:\n# It is possible to fall back to a static IP if DHCP fails:\r# define static profile\rprofile static_eth0\rstatic ip_address=10.10.10.2/24\rstatic routers=10.10.10.1\rstatic domain_name_servers=10.10.10.1\r# fallback to static profile on eth0\rinterface eth0\rfallback static_eth0\rWhere 10.10.10.1 is the routers IP address, and 10.10.10.2 is Pi-Holes IP address.\nIf you are using the wireless interface wlan0, replace instances of eth0 with wlan0. Don\u0026rsquo;t forget to check your compose file, DNSMASQ_LISTENING should be set to the interface you are using or set to all.\nThank you\rCheck out my home infrastructure repo to see what I\u0026rsquo;m running!\nhttps://github.com/alex-oswald/HomeInfra\nMore specifically for Pi-Hole!\nhttps://github.com/alex-oswald/HomeInfra/tree/main/pi-hole\nThat\u0026rsquo;s all for this post. Hopefully you enjoyed this quick tutorial on setting up Pi-Hole in Docker on a Raspberry Pi.\nIf you have any comments, please feel free to email me at alex@oswaldtechnologies.com.\n-Alex\n","troubleshooting#Troubleshooting":""},"title":"Pi-Hole in Docker on a Raspberry Pi"},"/blog/2021-11-17-outlook-rule-for-on-behalf-of-email/":{"data":{"":"","how-do-i-setup-an-outlook-rule-to-filter-on-behalf-of-emails#How do I setup an Outlook rule to filter \u003cem\u003eon behalf of\u003c/em\u003e emails?":"I get a lot of different alert emails, from Azure DevOps, AWS, etc. Setting up rules to move these emails to different folders is important for my sanity. Alert emails I typically get come from a central source, and the on behalf of email is from the root source, like Azure DevOps or AWS. This post answers the question:\nHow do I setup an Outlook rule to filter on behalf of emails?\rOpen the Rules and Alerts window in Outlook Click New Rule\u0026hellip; Click Next to skip to the advanced setup Unfortuantely, we can\u0026rsquo;t create a rule for on behalf of email address using the from people or public group. The address is stored in the email header though, so we can search the header for the address.\nSelect with specific words in the message header Click specific words in Step 2 to add the email address you want to filter on Click Ok to exit the Search Text dialog Select the folder you want to move emails to and click Finish "},"title":"Outlook Rule for 'on behalf of' Emails"},"/blog/2022-02-23-windows-forms-lifetime/":{"data":{"":"","conclusion#Conclusion":"This post will go over the Windows Forms Lifetime library I\u0026rsquo;ve started writing. The library is a Windows Forms hosting extension for the .NET Generic Host. It enables you to configure the generic host to use the lifetime of Windows Forms, aka when the main Form closes, the host will shut down.\nhttps://github.com/alex-oswald/WindowsFormsLifetime\nGet started with .NET 6\u0026rsquo;s Minimal API\rCreate a new Windows Forms App.\nChange the projects SDK to Microsoft.NET.Sdk.Web so we can use the WebApplication class.\nAdd NoDefaultLaunchSettingsFile to the csproj so a launchSettings.json file isn\u0026rsquo;t created automatically for us.\n\u0026lt;Project Sdk=\u0026#34;Microsoft.NET.Sdk.Web\u0026#34;\u0026gt; \u0026lt;PropertyGroup\u0026gt; \u0026lt;OutputType\u0026gt;WinExe\u0026lt;/OutputType\u0026gt; \u0026lt;TargetFramework\u0026gt;net6.0-windows\u0026lt;/TargetFramework\u0026gt; \u0026lt;Nullable\u0026gt;enable\u0026lt;/Nullable\u0026gt; \u0026lt;UseWindowsForms\u0026gt;true\u0026lt;/UseWindowsForms\u0026gt; \u0026lt;ImplicitUsings\u0026gt;enable\u0026lt;/ImplicitUsings\u0026gt; \u0026lt;NoDefaultLaunchSettingsFile\u0026gt;true\u0026lt;/NoDefaultLaunchSettingsFile\u0026gt; \u0026lt;/PropertyGroup\u0026gt; \u0026lt;/Project\u0026gt;\rReplace the contents of Program.cs with the following.\nusing WinFormsApp1; var builder = WebApplication.CreateBuilder(args); builder.Host.UseWindowsFormsLifetime\u0026lt;Form1\u0026gt;(); var app = builder.Build(); app.Run();\rInstantiating and Showing Forms\rAdd more forms to the DI container.\nusing WinFormsApp1; var builder = WebApplication.CreateBuilder(args); builder.Host.UseWindowsFormsLifetime\u0026lt;Form1\u0026gt;(); builder.Services.AddTransient\u0026lt;Form2\u0026gt;(); var app = builder.Build(); app.Run();\rTo get a form use the IFormProvider. The form provider instantiates an instance of the form from the DI container on the GUI thread. IFormProvider has one method, GetFormAsync\u0026lt;T\u0026gt; used to fetch a form instance.\nIn this example, we inject IFormProvider into the main form, and use that to instantiate a new instance of Form, then show the form. Instantiating a Form directly in a nother form will also create it on the GUI thread, but using the IFormProvider is best practice when using this library.\npublic partial class Form1 : Form { private readonly ILogger\u0026lt;Form1\u0026gt; _logger; private readonly IFormProvider _formProvider; public Form1(ILogger\u0026lt;Form1\u0026gt; logger, IFormProvider formProvider) { InitializeComponent(); _logger = logger; _formProvider = formProvider; } private async void button1_Click(object sender, EventArgs e) { _logger.LogInformation(\u0026#34;Show Form2\u0026#34;); var form = await _formProvider.GetFormAsync\u0026lt;Form2\u0026gt;(); form.Show(); } }\rInvoking on the GUI thread\rSometimes you need to invoke an action on the GUI thread. Say you want to spawn a form from a background service. Use the IGuiContext to invoke actions on the GUI thread.\nIn this example, a form is fetched and shown every 5 seconds 5 times, in an action that is invoked on the GUI thread. This example shows how the GUI thread does not lock up during this process.\npublic class HostedService1 : BackgroundService { private readonly IFormProvider _fp; private readonly IGuiContext _guiContext; public HostedService1( IFormProvider formProvider, IGuiContext guiContext) { _fp = formProvider; _guiContext = guiContext; } protected override async Task ExecuteAsync(CancellationToken stoppingToken) { int count = 0; while (!stoppingToken.IsCancellationRequested) { await Task.Delay(5000, stoppingToken); if (count \u0026lt; 5) { await _guiContext.InvokeAsync(async () =\u0026gt; { var form = await _fp.GetFormAsync\u0026lt;Form2\u0026gt;(); form.Show(); }); } count++; } } }\rConclusion\rI\u0026rsquo;ve explained that the WindowsFormsLifetime library lets you use .NET\u0026rsquo;s Generic Host with Windows Forms application, and how it can control the lifetime of the application. I demonstrated how to get started using .NET 6\u0026rsquo;s Minimal API. I also showed the two main services that come with the library, the IFormProvider which is used to new up forms on the GUI thread, and the IGuiContext which is used to invoke actions on the GUI thread to prevent cross-thread exceptions.\nIn a future post, I will demonstrate how to use the WindowsFormsLifetime.Mvp companion library to develop Model-View-Presenter applications using Windows Forms.\nUntil then, happy win forming!\n","get-started-with-net-6s-minimal-api#Get started with .NET 6\u0026rsquo;s Minimal API":"","instantiating-and-showing-forms#Instantiating and Showing Forms":"","invoking-on-the-gui-thread#Invoking on the GUI thread":""},"title":"Windows Forms Lifetime"},"/blog/2022-08-04-pc-build/":{"data":{"":"","3d-printer-slicing#3D Printer Slicing":"It\u0026rsquo;s time to build a beast of a development/gaming desktop PC in 2022!\rThe last computer I purchased was a Surface Book 2, over 4 years ago. It did an okay job for development, but it wasn\u0026rsquo;t great for gaming, and wasn\u0026rsquo;t VR ready. The biggest flaw for development was the RAM, with only 16GB. It wasn\u0026rsquo;t great running multiple instances of Visual Studio and multiple docker containers. There were countless times I found myself needing to close a VS instance to free up memory. For this build, I wanted a higher end desktop that would be able to tackle all the development I do, and be VR ready, and not completely break the bank.\nRequirements\rVR Ready (because Half-Life: Alyx of course) 12th Gen i9 32GB+ DDR5 (maybe 64GB) Under $3000 Bill of Materials\rHere\u0026rsquo;s what I came up with. Over the course of a month after I built the PC (while my return window was still good), a few components were on sale on Amazon so I ordered the components again and returned the new components under the old orders to get the cheaper price. I was able to save $80 on the GPU and $30 on the RAM this way. Further, I got 5% back using my Amazon Prime credit card, which brought the total to almost $300 less than my budget.\nComponent Price Case Lian Li LANCOOL II MESH PEROFRMANCE Black LAN2MPX Tempered Glass ATX Case $163.07 Motherboard ASUS Prime Z690-P $210.35 CPU Intel Core i9-12900K $559.99 RAM CORSAIR Vengeance DDR5 32GB (2x16GB) DDR5 5600 $229.99 GPU EVGA GeForce RTX 3080 FTW3 $779.99 Hard Drive SAMSUNG 980 PRO SSD with Heatsink 1TB PCIe Gen 4 NVMe M.2 $139.99 Power Supply Seasonic PRIME TX-850, 850W 80+ Titanium $259.99 Thermal Paste Arctic Silver 5 AS5-3.5G $7.90 CPU Cooler NZXT Kraken X73 360mm - RL-KRX73-01 $157.99 Vertical GPU Mount EZDIY-FAB Vertical PCIe 4.0 GPU Mount $59.99 Bluetooth Adapter Long Range Bluetooth Adapter for PC $14.39 BenchmarksI\u0026rsquo;m not a hardcore benchmarker, so I stuck to only UserBenchmark and a few real world tests.\nUserBenchmark\rView my latest results\nSo far I\u0026rsquo;ve only clocked my RAM at 5200MHz.\nUserBenchmarks Game 264%, Desk 119%, Work 300%\nCPU 119.2%\nGPU 227.8%\nSSD 401.5%\nRAM 177%\nVisual Studio Builds\rFor this benchmark I built one of my open source projects, WindowsFormsLifetime. This isn\u0026rsquo;t a big solution, but it\u0026rsquo;s a real world test because it represents builds I\u0026rsquo;m actually running multiple times throughout the day. Before running dotnet build, I cleaned out the obj and bin folders to represent a fresh build.\nPC 3 Run Avg New Desktop 1 seconds Surface Book 2 8 seconds 3D Printer Slicing\rFor this benchmark I used Ultimaker Cura to slice the Forbidden Tower with the default Standard Quality settings. I didn\u0026rsquo;t want to dive into setting up a program to accuratly time the slicing, so I went the old fashion route and used a stopwatch. I have to say, the time differences here were pretty incredible.\nPC 2 Run Avg New Desktop 7 seconds Surface Book 2 70 seconds ","benchmarks#Benchmarks":"","bill-of-materials#Bill of Materials":"","its-time-to-build-a-beast-of-a-developmentgaming-desktop-pc-in-2022#It\u0026rsquo;s time to build a beast of a development/gaming desktop PC in 2022!":"","requirements#Requirements":"","userbenchmark#UserBenchmark":"","visual-studio-builds#Visual Studio Builds":""},"title":"2022 Development/Gaming PC Build"},"/blog/2022-08-22-create-a-ssl-cert-for-your-home-network/":{"data":{"conclusion#Conclusion":"In this post I will explain how to create a SSL certificate for your home network and how to install the root cert onto your computers so your browsers trust it.\nWhy?For me, the need came after I created a docker cluster on one of my Raspberry Pi\u0026rsquo;s with a reverse proxy and wanted to expose those services with local DNS using the pi.lan domain. I didn\u0026rsquo;t want to enable port forwarding and expose my services to the internet.\nGenerate the Root Key\ropenssl genrsa -des3 -out root.key 4096\rEnter a new password when prompted.\npi@rp-400:~ $ openssl genrsa -des3 -out root.key 4096 Generating RSA private key, 4096 bit long modulus (2 primes) ....................++++ ..................++++ e is 65537 (0x010001) Enter pass phrase for root.key: Verifying - Enter pass phrase for root.key: pi@rp-400:~ $ ls root.key\rroot.key has now been created.\nCreate Root Certificate\rAdjust validity days to your liking. If you plan to access your websites with an iOS device, keep the days under 398. 1 year is a good go to.\nopenssl req -x509 -new -nodes -key root.key -sha256 -days 365 -out root.pem\rEnter your root key password when prompted. Then any other optional information. You can ENTER through each to skip.\npi@rp-400:~ $ openssl req -x509 -new -nodes -key root.key -sha256 -days 365 -out root.pem Enter pass phrase for root.key: You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter \u0026#39;.\u0026#39;, the field will be left blank. ----- Country Name (2 letter code) [AU]: State or Province Name (full name) [Some-State]: Locality Name (eg, city) []: Organization Name (eg, company) [Internet Widgits Pty Ltd]: Organizational Unit Name (eg, section) []: Common Name (e.g. server FQDN or YOUR name) []: Email Address []: pi@rp-400:~ $ ls root.key root.pem\rroot.pem has now been created.\nCreate CSR Configuration Files\rCreate a text file named v3.ext with the below contents. Add any domains to alt_names. In my case, I\u0026rsquo;ve added the wildcard for pi.lan (so the cert can be used with all subdomains), along with pi.lan it self.\nauthorityKeyIdentifier=keyid,issuer\rbasicConstraints=CA:FALSE\rkeyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment\rsubjectAltName = @alt_names\r[alt_names]\rDNS.1 = *.pi.lan\rDNS.2 = pi.lan\rCreate a text file named sslcert.csr.cnf with the below contents. Update the properties in the dn section with your values.\n[req]\rdefault_bits = 4096\rprompt = no\rdefault_md = sha256\rdistinguished_name = dn\r[dn]\rC=US\rST=WA\rL=Bellevue\rO=Oswald Technologies\rOU=Oswald Technologies\remailAddress=alex@oswaldtechnologies.com\rCN = Oswald Technologies LAN Cert\rpi@rp-400:~ $ ls root.key root.pem sslcert.csr.cnf v3.ext\rCreate Certificate-Signing Request or CSR\ropenssl req -new -sha256 -nodes -out sslcert.csr -newkey rsa:4096 -keyout sslcert.key -config sslcert.csr.cnf\rpi@rp-400:~ $ openssl req -new -sha256 -nodes -out sslcert.csr -newkey rsa:4096 -keyout sslcert.key -config sslcert.csr.cnf Generating a RSA private key ..............................................................................................................++++ .........................................................................................++++ writing new private key to \u0026#39;sslcert.key\u0026#39; ----- pi@rp-400:~/cert_test $ ls root.key root.pem sslcert.csr sslcert.csr.cnf sslcert.key v3.ext\rsslcert.csr and sslcert.key have now been created.\nCreate the SSL Certificate\rCreate .crt. In my case, this is the certificate file used by Traefik.\nopenssl x509 -req -in sslcert.csr -CA root.pem -CAkey root.key -CAcreateserial -out sslcert.crt -days 365 -sha256 -extfile v3.ext\rpi@rp-400:~ $ openssl x509 -req -in sslcert.csr -CA root.pem -CAkey root.key -CAcreateserial -out sslcert.crt -days 365 -sha256 -extfile v3.ext Signature ok subject=C = US, ST = WA, L = Bellevue, O = Oswald Technologies, OU = Oswald Technologies, emailAddress = alex@oswaldtechnologies.com, CN = Oswald Technologies LAN Cert Getting CA Private Key Enter pass phrase for root.key: pi@rp-400:~ $ ls root.key root.pem root.srl sslcert.crt sslcert.csr sslcert.csr.cnf sslcert.key v3.ext\rsslcert.crt and root.srl have now been created.\nCreate PFX File\rIf needed, you can create a .pfx file from the certificate. In my case, this is the certificate file used by my .NET web applications to enable HTTPS.\nopenssl pkcs12 -export -out sslcert.pfx -inkey sslcert.key -in sslcert.crt -passout pass:\rpi@rp-400:~ $ openssl pkcs12 -export -out sslcert.pfx -inkey sslcert.key -in sslcert.crt -passout pass: pi@rp-400:~ $ ls root.key root.pem root.srl sslcert.crt sslcert.csr sslcert.csr.cnf sslcert.key sslcert.pfx v3.ext\rTrusting the CertificatesWindows\rAdd the root CA to the Windows certificate store.\ncertutil -addstore -f \u0026#34;ROOT\u0026#34; root.pem\rOnce added, you will be able to see it in the Certificate Manager for Local Computer and Current User. It will be under Trusted Root Certification Authorities -\u0026gt; Certificates.\nLinux\rCreate a folder for your cert in ca-certificates.\nmkdir /usr/local/share/ca-certificates/\u0026lt;folder\u0026gt;\rCopy the pem file to this folder then update certificates.\nsudo update-ca-certificates\rYou will then need to manually trust the certificate in the browsers you use since the browsers use their own certificate store, and not the certficate store of the OS like in Windows.\niOS\rTransfer the pem file to your phone.\nOpen the pem file in the Files app and choose to install on the iPhone (if you have a watch there will be an option to install on the watch as well).\nGo to Settings \u0026gt; General \u0026gt; VPN \u0026amp; Device Management\nUnder Configuration Profile select the cert and click Install.\nGo to Settings \u0026gt; General \u0026gt; About \u0026gt; Certificate Trust Settings and trust the certificate.\nIf you picked a validity period above 398 days, you will not see the cert in Certificate Trust Settings from my experience, and will not be able to trust it.\nConclusionIn this post I explained how to create your own root CA and create an SSL certificate from that. I explained how to install the certificate on Windows, Linux and iOS. With these instructions, you can use HTTPS for web services running locally through a reverse proxy. Here\u0026rsquo;s some iOS screenshots of some apps I\u0026rsquo;m doing this with.\n","create-certificate-signing-request-or-csr#Create Certificate-Signing Request or CSR":"","create-csr-configuration-files#Create CSR Configuration Files":"","create-pfx-file#Create PFX File":"","create-root-certificate#Create Root Certificate":"","create-the-ssl-certificate#Create the SSL Certificate":"","generate-the-root-key#Generate the Root Key":"","ios#iOS":"","linux#Linux":"","trusting-the-certificates#Trusting the Certificates":"","why#Why?":"","windows#Windows":""},"title":"Create a SSL certificate for your home network"},"/blog/2022-11-25-create-a-ssl-cert-for-your-home-network-part2/":{"data":{"":"As a follow up to my previous post Create a SSL certificate for your home network, I\u0026rsquo;ve created a simple shell script you can to use to quickly create ssl certificates for a local domain.\nCreate a create_ssl.sh file with the following contents:\n# GENERATE SSL CERTIFICATE # pi.lan, *.lan # # PARAMETERS # 1: Root Password # 2: Validity in days # 3: PFX Password # # EXECUTION # chomd +x create_ssl.sh # ./create_ssl.sh rootkeypass 365 pfxpass echo \u0026#34;Generating RSA-2048 key file \u0026#39;root.key\u0026#39;\u0026#34; openssl genrsa -des3 -passout pass:$1 -out root.key 4096 echo \u0026#34;Generating SSL configuration file: \u0026#39;ssl.config\u0026#39;\u0026#34; cat \u0026gt;\u0026gt; ssl.config \u0026lt;\u0026lt;EOL default_bits = 4096 prompt = no default_md = sha256 distinguished_name = dn [dn] C=US ST=WA L=Seattle O=Local LAN Cert OU=Local LAN Cert emailAddress=admin@pi.lan CN = Local LAN Cert EOL echo \u0026#34;Generating Root Certificate file \u0026#39;root.pem\u0026#39;\u0026#34; openssl req -x509 -new -nodes -key root.key -sha256 -days $2 -out root.pem -passin pass:$1 -config ssl.config echo \u0026#34;Generating Certificate Extensions file \u0026#39;cert.ext\u0026#39;\u0026#34; cat \u0026gt;\u0026gt; cert.ext \u0026lt;\u0026lt;EOL authorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment subjectAltName = @alt_names [alt_names] DNS.1 = *.pi.lan DNS.2 = pi.lan EOL echo \u0026#34;Generating Certificate Signing Request (CSR) files \u0026#39;sslcert.csr\u0026#39;, \u0026#39;sslcert.key\u0026#39;\u0026#34; openssl req -new -sha256 -nodes -out sslcert.csr -newkey rsa:4096 -keyout sslcert.key -config ssl.config echo \u0026#34;Generating Certificate file \u0026#39;sslcert.crt\u0026#39;\u0026#34; openssl x509 -req -in sslcert.csr -CA root.pem -CAkey root.key -CAcreateserial -out sslcert.crt -days 365 -sha256 -extfile cert.ext -passin pass:$1 echo \u0026#34;Generating PFX file \u0026#39;sslcert.pfx\u0026#39;\u0026#34; openssl pkcs12 -export -out sslcert.pfx -inkey sslcert.key -in sslcert.crt -passout pass:$3\r"},"title":"Create a SSL certificate for your home network Part 2"},"/blog/2023-02-20-cloudflare-ddns/":{"data":{"":"","running-the-container#Running the container":"In this post I\u0026rsquo;m going to show you how to use the Cloudflare DDNS service I wrote to keep my constantly changing public IP updated with my VPN\u0026rsquo;s DNS record in Cloudflare. I use a NetGear Orbi router at home, and it supports using No-IP as a free DDNS service. The problem with using this free service, is you have to login and manually confirm your free hostname monthly. While this may be okay for those that don\u0026rsquo;t own any domain names, it isn\u0026rsquo;t something I wanted to keep doing manually.\nWhy do we need a DDNS service?\rThe reason for this, is because your ISP typically changes your public IP address every so often, unless you pay for a static IP, which is only common for businesses. And typically you want your IP address assigned to a hostname, so you can connect your VPN to something like, vpn.example.com, instead of the actual IP address. If you don\u0026rsquo;t, you would need to update your VPN profile on each device to point to the new IP address each time it changes. So how do we get around this? Dynamic DNS services, like the one my Orbi router supports, let you configure a free hostname, such as vpn.mynetgear.com to point to your public IP, and automatically update the DNS record each time the router detects a new public IP. But this comes at the cost of doing the manual hostname confirmation each month to keep it free.\nBecause I love to automate things, already own some domain names, and run a Docker swarm in my home lab, I wanted to put together a simples solution to the problem instead of finding another solution online. While I found a few that would work, they didn\u0026rsquo;t do exactly what I wanted, and I wouldn\u0026rsquo;t of learned as much. So here we are\u0026hellip;\nWhat is CloudflareDDNS\rSee the CloudflareDDNS repository on github: https://github.com/alex-oswald/CloudflareDDNS\nCloudflareDDNS is a .NET application that runs in a container. It creates a background service that runs a few commands on an interval, defaulting to every 15 minutes. You can run it locally on any machine. I run mine as a stack on a Raspberry Pi swarm since setup is simple with Portainer.\nThe DDNSBackgroundService will do the following:\nUse a DNS request to get your public IP address Get a list of Zones from your Cloudflare account Get the id of the Zone you have specified via config from the list of Zones Get the DNS records for the Zone If the DNS record you specified via config does not exist, it will create an A record with your public IP address If it does exist, and the IP addresses do not match, it will update the DNS record with the new IP address If the IP addresses do match, nothing will happen The background service delays for the configures time interval Running the container\rMake sure you have Docker Desktop installed.\nObtain an API token from Cloudflare with at least Zone Read and DNS Edit permissions.\nIf you use Portainer as well, you know what to do.\nOtherwise, create a docker-compose.yml file in a directory of your choosing with the following contents.\nversion: \u0026#39;3.8\u0026#39; services: cloudflareddns: image: \u0026#39;ghcr.io/alex-oswald/cloudflareddns:main\u0026#39; environment: CloudflareApi__ApiToken: api_token DDNS__ZoneName: example.com DDNS__DnsRecordName: vpn.example.com\rAdd your API token value, and proper Zone and DNS record names.\nTo change the update interval from the default 15 minutes, add another environment variables named DDNS__UpdateIntervalSeconds with a value of your choosing.\nIf you\u0026rsquo;ve come this far, you know to run the following to see it work.\ndocker-compose up\rYou will start seeing logs like the following.\n[08:29:16 INF] Starting CloudflareDDNS host\r[08:29:16 INF] DDNSBackgroundService service started\r[08:29:17 INF] Now listening on: http://[::]:80\r[08:29:17 INF] Application started. Press Ctrl\u0026#43;C to shut down.\r[08:29:17 INF] Hosting environment: Production\r[08:29:17 INF] Content root path: /app\r[08:29:18 INF] Public IP: *.*.*.*\r[08:29:18 DBG] Get zones uri=https://api.cloudflare.com/client/v4/zones\r[08:29:18 DBG] Get zone details uri=https://api.cloudflare.com/client/v4/zones/id\r[08:29:18 INF] Zone info: id=id, name=example.com\r[08:29:18 DBG] Get DNS records uri=https://api.cloudflare.com/client/v4/zones/id/dns_records\r[08:29:19 DBG] Found DNS record vpn.example.com\r[08:29:19 DBG] Your public IP matches the DNS record content. Nothing to update here. üëç\r[08:29:19 INF] Waiting 900 seconds till next update... üò¥\rEnjoy!\n","what-is-cloudflareddns#What is CloudflareDDNS":"","why-do-we-need-a-ddns-service#Why do we need a DDNS service?":""},"title":"Running a Cloudflare DDNS service for your home VPN"},"/blog/2023-10-08-azure-devops-artifacts/":{"data":{"":"If you\u0026rsquo;ve come here, you know what you want. You want to be able to download individual files from an Azure DevOps pipeline artifact. The documentation makes no sense. So lets get straight to the point.\nFirst, I want to point out that this only works with PublishPipelineArtifact, and NOT with PublishBuildArtifacts\nI\u0026rsquo;ve created a demo pipeline for this post. It simply creates two text files and publishes them as a pipeline artifact.\npool: vmImage: \u0026#39;ubuntu-latest\u0026#39; steps: - pwsh: | Write-Host \u0026#34;Test files\u0026#34; \u0026#34;Test string 1\u0026#34; | Out-File -FilePath .\\test1.txt \u0026#34;Test string 2\u0026#34; | Out-File -FilePath .\\test2.txt workingDirectory: $(Build.ArtifactStagingDirectory) - task: PublishPipelineArtifact@1 inputs: ArtifactName: TestArtifact targetPath: $(Build.ArtifactStagingDirectory)\rLet us start by hitting the Artifacts - Get Artifact so we can get some info about the artifact.\nRequest\nhttps://dev.azure.com/oswaldtechnologies/add3132d-b1ce-4519-8299-4e67eecad1f5/_apis/build/builds/703/artifacts?artifactName=TestArtifact\rResponse\n{ \u0026#34;id\u0026#34;: 322, \u0026#34;name\u0026#34;: \u0026#34;TestArtifact\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;12f1170f-54f2-53f3-20dd-22fc7dff55f9\u0026#34;, \u0026#34;resource\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;PipelineArtifact\u0026#34;, \u0026#34;data\u0026#34;: \u0026#34;973C2055701973A0FDFB695031EE3F9E7A91741016CA639E9D21ECCD1B387E9B01\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;RootId\u0026#34;: \u0026#34;DC04A61FBB2E879A10EE8BA01B28B4623140546805A968AB2B491B2EE1BD2E4102\u0026#34;, \u0026#34;artifactsize\u0026#34;: \u0026#34;28\u0026#34;, \u0026#34;HashType\u0026#34;: \u0026#34;DEDUPNODEORCHUNK\u0026#34; }, \u0026#34;url\u0026#34;: \u0026#34;https://dev.azure.com/oswaldtechnologies/add3132d-b1ce-4519-8299-4e67eecad1f5/_apis/build/builds/703/artifacts?artifactName=TestArtifact\u0026amp;api-version=7.1\u0026#34;, \u0026#34;downloadUrl\u0026#34;: \u0026#34;https://artprodeus21.artifacts.visualstudio.com/A3e090689-466b-408e-a12e-87c169eff347/add3132d-b1ce-4519-8299-4e67eecad1f5/_apis/artifact/cGlwZWxpbmVhcnRpZmFjdDovL29zd2FsZHRlY2hub2xvZ2llcy9wcm9qZWN0SWQvYWRkMzEzMmQtYjFjZS00NTE5LTgyOTktNGU2N2VlY2FkMWY1L2J1aWxkSWQvNzAzL2FydGlmYWN0TmFtZS9UZXN0QXJ0aWZhY3Q1/content?format=zip\u0026#34; } }\rNotice resource.data. This is actually the fileId. And this is what the documentation does not mention. We use the fileId of the actual artifact, to get the artifacts manifest contents. The manifest contains an array of all items in the artifact, including their file path and file id. So now lets hit Aritfacts - Get File with the artifacts file id to get the manifest. Oddly, you must add fileId and fileName to the query, even though what you set fileName to doesn\u0026rsquo;t matter. For this example, I\u0026rsquo;m going to call it manifest.json, because well, that\u0026rsquo;s what it is.\nRequest\nhttps://dev.azure.com/oswaldtechnologies/add3132d-b1ce-4519-8299-4e67eecad1f5/_apis/build/builds/703/artifacts?artifactName=TestArtifact\u0026amp;fileId=973C2055701973A0FDFB695031EE3F9E7A91741016CA639E9D21ECCD1B387E9B01\u0026amp;fileName=manifest.json\rResponse\n{ \u0026#34;manifestFormat\u0026#34;: \u0026#34;1.1.0\u0026#34;, \u0026#34;items\u0026#34;: [ { \u0026#34;path\u0026#34;: \u0026#34;/test1.txt\u0026#34;, \u0026#34;blob\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;D21C967E56201F344B44EE00F537263C3503AEB13931F99754F9E78E14E6C90C01\u0026#34;, \u0026#34;size\u0026#34;: 14 } }, { \u0026#34;path\u0026#34;: \u0026#34;/test2.txt\u0026#34;, \u0026#34;blob\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;FE2A48E456C37C7BAF1F86D724E2C2B30658AA1A899201D61E23CE59A333A63801\u0026#34;, \u0026#34;size\u0026#34;: 14 } } ], \u0026#34;manifestReferences\u0026#34;: [] }\rNow we can hit Aritfacts - Get File once more. This time let us get the contents of test1.txt. Again, it does not matter what you set fileName too, as long as the fileId matches the blob.id of the file you want to download.\nRequest\nhttps://dev.azure.com/oswaldtechnologies/add3132d-b1ce-4519-8299-4e67eecad1f5/_apis/build/builds/703/artifacts?artifactName=TestArtifact\u0026amp;fileId=D21C967E56201F344B44EE00F537263C3503AEB13931F99754F9E78E14E6C90C01\u0026amp;fileName=test1.txt\rResponse\nTest string 1\rAnd there you have it! Figuring out how to do this was important for a project I was working on. I needed to download a small json file from artifacts that were over 10GB in size. It would be silly to download the whole artifact just to extract a small json file from it.\nGive credit where credit is due. I found the solution to this problem via this GitHub issues comment.\n{{ partial \u0026ldquo;giscus/script\u0026rdquo; . }}\n"},"title":"Downloading individual files from an Azure DevOps pipeline artifact using the REST API"}}